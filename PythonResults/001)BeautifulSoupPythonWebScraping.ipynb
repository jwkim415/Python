{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<body>\n",
      "<div class=\"ecopyramid\">\n",
      "<ul id=\"producers\">\n",
      "<li class=\"producerlist\">\n",
      "<div class=\"name\">plants</div>\n",
      "<div class=\"number\">100000</div>\n",
      "</li>\n",
      "<li class=\"producerlist\">\n",
      "<div class=\"name\">algae</div>\n",
      "<div class=\"number\">100000</div>\n",
      "</li>\n",
      "</ul>\n",
      "<ul id=\"primaryconsumers\">\n",
      "<li class=\"primaryconsumerlist\">\n",
      "<div class=\"name\">deer</div>\n",
      "<div class=\"number\">1000</div>\n",
      "</li>\n",
      "<li class=\"primaryconsumerlist\">\n",
      "<div class=\"name\">rabbit</div>\n",
      "<div class=\"number\">2000</div>\n",
      "</li>\n",
      "</ul>\n",
      "<ul id=\"secondaryconsumers\">\n",
      "<li class=\"secondaryconsumerlist\">\n",
      "<div class=\"name\">fox</div>\n",
      "<div class=\"number\">100</div>\n",
      "</li>\n",
      "<li class=\"secondaryconsumerlist\">\n",
      "<div class=\"name\">bear</div>\n",
      "<div class=\"number\">100</div>\n",
      "</li>\n",
      "</ul>\n",
      "<ul id=\"tertiaryconsumers\">\n",
      "<li class=\"tertiaryconsumerlist\">\n",
      "<div class=\"name\">lion</div>\n",
      "<div class=\"number\">80</div>\n",
      "</li>\n",
      "<li class=\"tertiaryconsumerlist\">\n",
      "<div class=\"name\">tiger</div>\n",
      "<div class=\"number\">50</div>\n",
      "</li>\n",
      "</ul>\n",
      "</div></body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "#beautifulsoup 연습-1\n",
    "from bs4 import BeautifulSoup\n",
    "f=open(\"d:\\\\data\\\\ecologicalpyramid.html\")\n",
    "soup=BeautifulSoup(f,\"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"name\">plants</div>, <div class=\"name\">algae</div>, <div class=\"name\">deer</div>, <div class=\"name\">rabbit</div>, <div class=\"name\">fox</div>, <div class=\"name\">bear</div>, <div class=\"name\">lion</div>, <div class=\"name\">tiger</div>]\n"
     ]
    }
   ],
   "source": [
    "#ecologicalpyramid에서 class이름 name에 접근해서 데이터를 긁어오시오\n",
    "from bs4 import BeautifulSoup\n",
    "f=open(\"d:\\\\data\\\\ecologicalpyramid.html\")\n",
    "soup=BeautifulSoup(f,\"html.parser\")\n",
    "result=soup.find_all(class_ =\"name\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plants\n",
      "algae\n",
      "deer\n",
      "rabbit\n",
      "fox\n",
      "bear\n",
      "lion\n",
      "tiger\n"
     ]
    }
   ],
   "source": [
    "#위에서 긁어온 데이터에서 html코드말고 text만 출력하기\n",
    "from bs4 import BeautifulSoup\n",
    "f=open(\"d:\\\\data\\\\ecologicalpyramid.html\")\n",
    "soup=BeautifulSoup(f,\"html.parser\")\n",
    "result=soup.find_all(class_ =\"name\")\n",
    "for i in result: #result리스트 안의 요소를 하나씩 가져옵니다.\n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 80, 100, 100, 1000, 2000, 100000, 100000]\n"
     ]
    }
   ],
   "source": [
    "#Q.ecologicalpyramid에서 class 이름 number의 숫자만 가져와서 a라는 비어있는 리스트에 \n",
    "#저장한 후 a안의 요소들을 정렬하고 a리스트를 출력하시오\n",
    "from bs4 import BeautifulSoup\n",
    "f=open(\"d:\\\\data\\\\ecologicalpyramid.html\")\n",
    "soup=BeautifulSoup(f,\"html.parser\")\n",
    "result=soup.find_all(class_ =\"number\")\n",
    "a=[]\n",
    "for i in result: #result리스트 안의 요소를 하나씩 가져옵니다.\n",
    "    a.append(int(i.get_text()))\n",
    "a.sort()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.중앙일보에서 기사 하나를 html로 저장하고 파싱하시오\n",
    "from bs4 import BeautifulSoup\n",
    "f=open(\"d:\\\\data\\\\aa77.html\", encoding='UTF8')\n",
    "soup=BeautifulSoup(f,\"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.위의 기사에서 본문에 해당하는 class 이름을 찾아서 해당 클래스의 text를 긁어오시오\n",
    "from bs4 import BeautifulSoup\n",
    "f=open(\"d:\\\\data\\\\aa77.html\", encoding='UTF8')\n",
    "soup=BeautifulSoup(f,\"html.parser\")\n",
    "result=soup.find_all(class_ =\"article_body mg fs4\")\n",
    "for i in result: #result리스트 안의 요소를 하나씩 가져옵니다.\n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.중앙일보에서 기사를 하나 스크롤링 해오고 mytext23.txt라는 파일을 생성하시오\n",
    "from bs4 import BeautifulSoup\n",
    "f=open(\"d:\\\\data\\\\aa77.html\", encoding='UTF8')\n",
    "soup=BeautifulSoup(f,\"html.parser\")\n",
    "result=soup.find_all(class_ =\"article_body mg fs4\")\n",
    "a=[]\n",
    "for i in result: #result리스트 안의 요소를 하나씩 가져옵니다.\n",
    "    a.append(i.get_text())\n",
    "    \n",
    "with open(\"d:\\\\data\\\\mytext23.txt\",\"w\", encoding='UTF8') as f:#mydata2.txt를 생성하겠다.\n",
    "    data=f.write(str(a)) #result에 있는 내용을 문자열로 변환해서 mydata2.txt로 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "기사 출처: 김정민 . “[팩플]'베테랑 전문의' AI 의사, 한국은 100대 기업에 못 든다고?” 중앙일보, 중앙일보, 14 Dec. 2020, news.joins.com/article/23945435. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
